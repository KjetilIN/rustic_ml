\documentclass{article}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=red]{hyperref}
\begin{document}
    \section{Perceptron Theory}

    Perceptron is also known as a single neuron. 
    It is used as a binary classifier, and is a type of supervised learning. After training on data, it is able to classify a given input to one of two classes. 

    It was introduced in 1943 by Warren McCulloch and Walter Pitts.
    They released a paper in 1958 with all the details of the perceptron \url{https://psycnet.apa.org/doiLanding?doi=10.1037%2Fh0042519}.
    

    \subsection{Requirements}

    There is limitation for using the perceptron:

    \begin{itemize}
        \item Binary classification only - i.e only two classes in the dataset.
        \item Training data must be labeled.
        \item Data has to be linearly separable.
    \end{itemize}

    \subsection{Definition}

    The perceptron can be defined as a function $f(\vec{x})$, that take a feature vector $\vec{x}$:

    \begin{align}
        f(\vec{x}) &= h(\vec{w} \cdot \vec{x} + b) \\
                   &= h(w_1 \cdot x_1 + w_2 \cdot x_2 + b)
    \end{align}

    Where $\vec{w}$ is the weight vector with the two weights for the perceptron and $b$ is the bias of the network.
    
    Note that we use a activation function called \textit{Heaviside step function}. 
    The output of the activation function is either 0 or 1. 

    \subsection{Why do we need a bias?}

    The bias is important to improve the flexibility of the model. 
    Without a bias, the model will always go through origin. 
    When we introduce a bias, it allows the model to pass thought the x-axis at different points. 
    

    \subsection{Training}

    \begin{enumerate}
        \item Initialize weights.
        \item Loop over each training instance until some criteria is met.
        \item Calculate the output of the training instance, $y$.
        \item Compare the target value, $t$ to the output $y$.
        \item If $t = y$, then continue. If not, we need to change all the weights. 
        \begin{enumerate}
            \item If $t=0, y = 1$, we need increase the weights: $w_i = w_i + \eta(t-y)x_i$
            \item If $t=1, y = 0$, we need to decrease the weight: $w_i = w_i - \eta(y-t)x_i$
        \end{enumerate}
    \end{enumerate}


    \subsection{Perceptron Convergence Theorem}

    If the dataset is linearly separable, then the perceptron will eventually find a solution for the binary classification.
    Unless the training rate $\eta$ is to high. It is important to note that there could be more than one solution. 

\end{document}